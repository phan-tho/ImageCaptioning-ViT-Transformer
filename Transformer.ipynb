{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Based on Transformer\n",
    "* Encoder: ViT\n",
    "* Decoder: Original Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder ViT\n",
    "<div style=\"display: flex;\">\n",
    "  <img src=\"Img/ViT.png\" alt=\"ViT\" style=\"width: 80%;\">\n",
    "  <img src=\"Img/Decoder.png\" alt=\"Decoder\" style=\"width: 20%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding\n",
    "```\n",
    "    Separate original image to n patches (tokens)\n",
    "    Convert tokens to 1D tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 512):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img: (batch_size, in_channels, img_size, img_size)\n",
    "        img = self.conv(img)            # (batch_size, embed_dim,  img_size // batch_size, img_size // batch_size)\n",
    "        img = img.flatten(2)            # (batch_size, embed_dim, num_patches)\n",
    "        img = img.transpose(1, 2)       # (batch_size, num_patches, embed_dim)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test PatchEmbedding\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "out = PatchEmbedding(96, 16, 3, 512)(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "```\n",
    "    Sublayer 1:\n",
    "        Residual connection\n",
    "        Layer Norm\n",
    "        Multi Head Attention\n",
    "    Sublayer 2:\n",
    "        Residual connection\n",
    "        Layer Norm\n",
    "        MLP (With dropout)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim = 512, num_heads = 8, mlp_hidden_size = 1024, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(mlp_hidden_size, embed_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, num_patches, embed_dim)\n",
    "        normed_X = self.norm1(X)\n",
    "        X = X + self.attention(normed_X, normed_X, normed_X, need_weights=False)[0]\n",
    "        # attention returns output and attention weights\n",
    "\n",
    "        normed_X = self.norm2(X)\n",
    "        return X + self.mlp(normed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test EncoderBlock\n",
    "X = torch.ones((4, 36, 512))\n",
    "encoder = EncoderBlock()\n",
    "\n",
    "out = encoder(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer\n",
    "```\n",
    "    Patch Embedding\n",
    "    Position Embedding\n",
    "    Encoder (Fed one by one to stack)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_dim=512, \n",
    "                 embed_dropout=0.1, num_blocks=2, num_heads=8, mlp_hidden_size=1024, mlp_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "\n",
    "        # pos_embedding is same for every batch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2, embed_dim))\n",
    "        self.embed_dropout = nn.Dropout(embed_dropout)\n",
    "\n",
    "        # Encoder\n",
    "        modules = [EncoderBlock(embed_dim, num_heads, mlp_hidden_size, mlp_dropout)] * num_blocks\n",
    "        self.encoder_blocks = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, in_channels, img_size, img_size)\n",
    "        X = self.patch_embedding(X)         # (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            X[i] += self.pos_embedding[0]\n",
    "        X = self.embed_dropout(X)\n",
    "\n",
    "        X = self.encoder_blocks(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test ViT\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "model = ViT(96, 16, 512)\n",
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(3, 3), diagonal=1).bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim = 512, num_heads = 8, dropout = 0.1, mlp_hidden_size = 1024):\n",
    "        super().__init__()\n",
    "        self.mask_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.lNorm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.e_d_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.lNorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_size, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.lNorm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, X, encoder_output):\n",
    "        # X: (batch_size, num_steps, embed_dim)\n",
    "        num_steps = X.shape[1]\n",
    "\n",
    "        # mask to calculate attention only for first num_steps + 1 steps\n",
    "        mask = torch.triu(torch.ones(num_steps, num_steps), diagonal=1).bool()\n",
    "        self_attended = self.mask_attention(X, X, X, attn_mask=mask)\n",
    "        X = self.lNorm1(X + self_attended[0])\n",
    "\n",
    "        # encoder-decoder attention\n",
    "        e_d_attended = self.e_d_attention(X, encoder_output, encoder_output)\n",
    "        X = self.lNorm2(X + e_d_attended[0])\n",
    "\n",
    "        # mlp\n",
    "        X = self.lNorm3(X + self.mlp(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test decoder block\n",
    "X = torch.ones((4, 36, 512))\n",
    "encoder_output = torch.ones((4, 36, 512))\n",
    "decoder = DecoderBlock()\n",
    "out = decoder(X, encoder_output)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "$$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_encoding = self.pos_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.pos_encoding[:, :X.shape[1]]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_blocks=2,\n",
    "                  num_heads=8, mlp_hidden_size=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "\n",
    "        # Decoder Blocks\n",
    "        modules = [DecoderBlock(embed_dim, num_heads, dropout, mlp_hidden_size)] * num_blocks\n",
    "        self.decoder_blocks = nn.Sequential(*modules)\n",
    "\n",
    "        # output layer\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, X, encoder_output):\n",
    "        # X: (batch_size, num_steps)\n",
    "        # encoder_output: (batch_size, num_patches, embed_dim)\n",
    "        X = self.embedding(X)           # (batch_size, num_steps, embed_dim)\n",
    "\n",
    "        X = self.pos_encoding(X)        # (batch_size, num_steps, embed_dim)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            X = block(X, encoder_output)        # (batch_size, num_steps, embed_dim)\n",
    "\n",
    "        return self.fc(X)           # (batch_size, num_steps, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 10000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Decoder\n",
    "vocab_size, num_steps = 10000, 20\n",
    "\n",
    "X = torch.randint(0, vocab_size, (4, num_steps))\n",
    "encoder_output = torch.ones((4, 36, 512))\n",
    "decoder = Decoder(vocab_size)\n",
    "\n",
    "out = decoder(X, encoder_output)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8685, 1270, 4086, 4023])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator(output, vocab_size):\n",
    "    # output: (batch_size, num_steps, vocab_size)\n",
    "    return torch.argmax(output[:, -1], dim=-1)      # (batch_size, )\n",
    "\n",
    "generator(out, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "```\n",
    "    Combine Encoder and Decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, vocab_size, embed_dim=512, num_blocks=2,\n",
    "                  num_heads=8, mlp_hidden_size=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vit = ViT(\n",
    "            img_size, patch_size, embed_dim, num_blocks=num_blocks, \n",
    "            num_heads=num_heads, mlp_hidden_size=mlp_hidden_size, mlp_dropout=dropout\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size, embed_dim, num_blocks, \n",
    "            num_heads, mlp_hidden_size, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, img, state):\n",
    "        # img: (batch_size, in_channels, img_size, img_size)\n",
    "        # target: (batch_size, num_steps)\n",
    "        encoder_output = self.vit(img)                  # (batch_size, num_patches, embed_dim)\n",
    "        return self.decoder(state, encoder_output)     # (batch_size, num_steps, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 20, 10000]), torch.Size([4, 10000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Transformer\n",
    "vocab_size, num_steps = 10000, 20\n",
    "\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "state = torch.randint(0, vocab_size, (4, num_steps))\n",
    "\n",
    "model = Transformer(96, 16, vocab_size)\n",
    "out = model(X, state)\n",
    "out.shape, out[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2832, 5690, 5770, 6117])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(out, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 training step\n",
    "\n",
    "crossEntropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0104, -0.0165,  0.0270,  ..., -0.0361, -0.0040,  0.0413],\n",
       "        [-0.0122,  0.0017, -0.0080,  ..., -0.0196,  0.0353,  0.0357],\n",
       "        [ 0.0230,  0.0320, -0.0098,  ...,  0.0440,  0.0370, -0.0402],\n",
       "        ...,\n",
       "        [ 0.0017, -0.0116,  0.0152,  ...,  0.0229,  0.0224, -0.0006],\n",
       "        [-0.0034,  0.0403,  0.0371,  ..., -0.0430,  0.0344,  0.0219],\n",
       "        [ 0.0412,  0.0178,  0.0245,  ..., -0.0088,  0.0239, -0.0011]],\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(96, 16, vocab_size)\n",
    "origin_fc = model.decoder.fc.weight.clone()\n",
    "origin_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 26]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 26\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "state = torch.randint(0, vocab_size, (4, num_steps))\n",
    "label = torch.ones((4)).type(torch.LongTensor)\n",
    "\n",
    "model.train()\n",
    "out = model(X, state)\n",
    "\n",
    "print(out[:, -1, :].shape, label.shape)\n",
    "\n",
    "# crossEntropy((batch_size, class), (batch_size))\n",
    "loss = crossEntropy(out[:, -1, :], label)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the weights are updated\n",
    "model.decoder.fc.weight - origin_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9168, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2, 3), dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor([1, 2])\n",
    "\n",
    "loss = crossEntropy(a, b)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.0342, grad_fn=<NllLossBackward0>),\n",
       " tensor([[ 1.2051, -0.0831,  1.9851,  0.1464,  1.1750],\n",
       "         [ 1.4357,  0.7721, -2.0237,  0.0232,  0.0675],\n",
       "         [-1.2174,  0.0107,  0.2336,  0.1041, -1.1506]], requires_grad=True),\n",
       " tensor([1, 0, 4]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(input, target)\n",
    "\n",
    "output, input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
