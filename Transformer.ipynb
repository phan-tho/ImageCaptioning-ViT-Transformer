{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Based on Transformer\n",
    "* Encoder: ViT\n",
    "* Decoder: Original Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder ViT\n",
    "<div style=\"display: flex;\">\n",
    "  <img src=\"Img/ViT.png\" alt=\"ViT\" style=\"width: 80%;\">\n",
    "  <img src=\"Img/Decoder.png\" alt=\"Decoder\" style=\"width: 20%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding\n",
    "```\n",
    "    Separate original image to n patches (tokens)\n",
    "    Convert tokens to 1D tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels = 3, embed_dim = 512):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img: (batch_size, in_channels, img_size, img_size)\n",
    "        img = self.conv(img)            # (batch_size, embed_dim,  img_size // batch_size, img_size // batch_size)\n",
    "        img = img.flatten(2)            # (batch_size, embed_dim, num_patches)\n",
    "        img = img.transpose(1, 2)       # (batch_size, num_patches, embed_dim)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test PatchEmbedding\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "out = PatchEmbedding(96, 16, 3, 512)(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "```\n",
    "    Sublayer 1:\n",
    "        Residual connection\n",
    "        Layer Norm\n",
    "        Multi Head Attention\n",
    "    Sublayer 2:\n",
    "        Residual connection\n",
    "        Layer Norm\n",
    "        MLP (With dropout)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim = 512, num_heads = 8, mlp_hidden_size = 1024, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(mlp_hidden_size, embed_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, num_patches, embed_dim)\n",
    "        normed_X = self.norm1(X)\n",
    "        X = X + self.attention(normed_X, normed_X, normed_X, need_weights=False)[0]\n",
    "        # attention returns output and attention weights\n",
    "\n",
    "        normed_X = self.norm2(X)\n",
    "        return X + self.mlp(normed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test EncoderBlock\n",
    "X = torch.ones((4, 36, 512))\n",
    "encoder = EncoderBlock()\n",
    "\n",
    "out = encoder(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer\n",
    "```\n",
    "    Patch Embedding\n",
    "    Position Embedding\n",
    "    Encoder (Fed one by one to stack)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, \n",
    "                 embed_dim=512, embed_dropout=0.1, \n",
    "                 num_blocks=2, num_heads=8, mlp_hidden_size=1024, mlp_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "\n",
    "        # pos_embedding is same for every batch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2, embed_dim))\n",
    "        self.embed_dropout = nn.Dropout(embed_dropout)\n",
    "\n",
    "        # Encoder\n",
    "        modules = [EncoderBlock(embed_dim, num_heads, mlp_hidden_size, mlp_dropout)] * num_blocks\n",
    "        self.encoder_blocks = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (batch_size, in_channels, img_size, img_size)\n",
    "        X = self.patch_embedding(X)         # (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            X[i] += self.pos_embedding[0]\n",
    "        X = self.embed_dropout(X)\n",
    "\n",
    "        X = self.encoder_blocks(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test ViT\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "model = ViT(96, 16, 512)\n",
    "out = model(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(3, 3), diagonal=1).bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim = 512, num_heads = 8, dropout = 0.1, mlp_hidden_size = 1024):\n",
    "        super().__init__()\n",
    "        self.mask_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.lNorm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.e_d_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.lNorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_size, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.lNorm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, X, encoder_output):\n",
    "        # X: (batch_size, num_steps, embed_dim)\n",
    "        num_steps = X.shape[1]\n",
    "\n",
    "        # mask to calculate attention only for first num_steps + 1 steps\n",
    "        mask = torch.triu(torch.ones(num_steps, num_steps), diagonal=1).bool()\n",
    "        self_attended = self.mask_attention(X, X, X, attn_mask=mask)\n",
    "        X = self.lNorm1(X + self_attended[0])\n",
    "\n",
    "        # encoder-decoder attention\n",
    "        e_d_attended = self.e_d_attention(X, encoder_output, encoder_output)\n",
    "        X = self.lNorm2(X + e_d_attended[0])\n",
    "\n",
    "        # mlp\n",
    "        X = self.lNorm3(X + self.mlp(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test decoder block\n",
    "X = torch.ones((4, 36, 512))\n",
    "encoder_output = torch.ones((4, 36, 512))\n",
    "decoder = DecoderBlock()\n",
    "out = decoder(X, encoder_output)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "$$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_encoding = self.pos_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.pos_encoding[:, :X.shape[1]]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_blocks=2,\n",
    "                  num_heads=8, mlp_hidden_size=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "\n",
    "        # Decoder Blocks\n",
    "        modules = [DecoderBlock(embed_dim, num_heads, dropout, mlp_hidden_size)] * num_blocks\n",
    "        self.decoder_blocks = nn.Sequential(*modules)\n",
    "\n",
    "        # output layer\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, X, encoder_output):\n",
    "        # X: (batch_size, num_steps)\n",
    "        # encoder_output: (batch_size, num_patches, embed_dim)\n",
    "        X = self.embedding(X)           # (batch_size, num_steps, embed_dim)\n",
    "\n",
    "        X = self.pos_encoding(X)        # (batch_size, num_steps, embed_dim)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            X = block(X, encoder_output)        # (batch_size, num_steps, embed_dim)\n",
    "\n",
    "        return self.fc(X)           # (batch_size, num_steps, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 10000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Decoder\n",
    "vocab_size, num_steps = 10000, 20\n",
    "\n",
    "X = torch.randint(0, vocab_size, (4, num_steps))\n",
    "encoder_output = torch.ones((4, 36, 512))\n",
    "decoder = Decoder(vocab_size)\n",
    "\n",
    "out = decoder(X, encoder_output)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8685, 1270, 4086, 4023])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator(output, vocab_size):\n",
    "    # output: (batch_size, num_steps, vocab_size)\n",
    "    return torch.argmax(output[:, -1], dim=-1)      # (batch_size, )\n",
    "\n",
    "generator(out, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "```\n",
    "    Combine Encoder and Decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, vocab_size, embed_dim=512, num_blocks=2,\n",
    "                  num_heads=8, mlp_hidden_size=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vit = ViT(\n",
    "            img_size, patch_size, embed_dim, num_blocks=num_blocks, \n",
    "            num_heads=num_heads, mlp_hidden_size=mlp_hidden_size, mlp_dropout=dropout\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size, embed_dim, num_blocks, \n",
    "            num_heads, mlp_hidden_size, dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, img, state):\n",
    "        # img: (batch_size, in_channels, img_size, img_size)\n",
    "        # target: (batch_size, num_steps)\n",
    "        encoder_output = self.vit(img)                  # (batch_size, num_patches, embed_dim)\n",
    "        return self.decoder(state, encoder_output)     # (batch_size, num_steps, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 20, 10000]), torch.Size([4, 10000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Transformer\n",
    "vocab_size, num_steps = 10000, 20\n",
    "\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "state = torch.randint(0, vocab_size, (4, num_steps))\n",
    "\n",
    "model = Transformer(96, 16, vocab_size)\n",
    "out = model(X, state)\n",
    "out.shape, out[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2832, 5690, 5770, 6117])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(out, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 training step\n",
    "\n",
    "crossEntropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 26]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 26\n",
    "X = torch.ones((4, 3, 96, 96))\n",
    "state = torch.randint(0, vocab_size, (4, num_steps))\n",
    "label = torch.ones((4)).type(torch.LongTensor)\n",
    "\n",
    "model.train()\n",
    "out = model(X, state)\n",
    "\n",
    "print(out[:, -1, :].shape, label.shape)\n",
    "\n",
    "# crossEntropy((batch_size, class), (batch_size))\n",
    "loss = crossEntropy(out[:, -1, :], label)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Image to Tensor have shape (channels, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image to tensor\n",
    "def image_to_tensor(image_path):\n",
    "    \"\"\"Converts an image to a tensor with shape (channels, image_width, image_height).\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img = img.convert('RGB')  # Ensure 3 channels\n",
    "        img_array = np.array(img)\n",
    "        tensor = torch.tensor(img_array, dtype=torch.float32).permute(2, 0, 1)\n",
    "        return tensor\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle text\n",
    "```\n",
    "    Clean Text\n",
    "    Build Vocab\n",
    "    Convert to Tensor have shape (length, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
